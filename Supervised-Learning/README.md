# Supervised Learning ğŸ“˜

Supervised Learning is one of the most fundamental types of Machine Learning. In this paradigm, the model is trained on a **labeled dataset**, which means every input in the dataset has a corresponding correct output (label).

The goal of supervised learning is to **learn a mapping function** from inputs to outputs, so that the model can make accurate predictions on new, unseen data.

---

## ğŸ”‘ Key Concepts

- **Input (X)**: Features or independent variables used to make predictions.
- **Output (y)**: Target or dependent variable we are trying to predict.
- **Training Data**: Labeled data used to train the model.
- **Testing/Validation Data**: Used to evaluate the performance of the model.
- **Loss Function**: Measures the difference between predicted and actual values.

---

## ğŸ“‚ Types of Supervised Learning

### 1. Regression ğŸ“ˆ
Regression is used when the target variable is **continuous** (e.g., price, temperature, salary).

#### âœ… Algorithms Covered:

- **Linear Regression**: Models a linear relationship between inputs and target.
- **Polynomial Regression**: Models a non-linear relationship using polynomial terms.
- **Ridge & Lasso Regression**: Regularized versions of linear regression to reduce overfitting.
- **Support Vector Regression (SVR)**: Uses support vectors to predict continuous values.

#### ğŸ“Œ Example Use Cases:
- Predicting house prices
- Forecasting stock prices
- Estimating temperature

---

### 2. Classification ğŸ§ 
Classification is used when the target variable is **categorical** (e.g., spam/ham, yes/no, digit class).

#### âœ… Algorithms Covered:

- **Logistic Regression**: A probabilistic model for binary classification.
- **K-Nearest Neighbors (KNN)**: Classifies based on the labels of the nearest data points.
- **Decision Tree Classifier**: Uses a tree-like structure to make decisions.
- **Random Forest Classifier**: An ensemble of decision trees for better performance.
- **Support Vector Machine (SVM)**: Maximizes the margin between classes.
- **Naive Bayes**: A probabilistic classifier based on Bayesâ€™ theorem.

#### ğŸ“Œ Example Use Cases:
- Email spam detection
- Disease diagnosis (positive/negative)
- Handwritten digit recognition (MNIST)

---

## ğŸ“Œ Folder Structure

```
Supervised-Learning/ â”œâ”€â”€ Regression/ â”‚ â”œâ”€â”€ LinearRegression.ipynb â”‚ â”œâ”€â”€ PolynomialRegression.ipynb â”‚ â”œâ”€â”€ RidgeRegression.ipynb â”‚ â””â”€â”€ SVR.ipynb â”œâ”€â”€ Classification/ â”‚ â”œâ”€â”€ LogisticRegression.ipynb â”‚ â”œâ”€â”€ KNN.ipynb â”‚ â”œâ”€â”€ DecisionTree.ipynb â”‚ â”œâ”€â”€ RandomForest.ipynb â”‚ â”œâ”€â”€ SVM.ipynb â”‚ â””â”€â”€ NaiveBayes.ipynb â””â”€â”€ README.md

```

---

## ğŸ§ª What's Inside Each Notebook?

- Explanation of the algorithm
- Step-by-step implementation using Jupyter Notebook
- Data loading and preprocessing
- Model training and evaluation (accuracy, confusion matrix, etc.)
- Visualization of results where applicable

---

## ğŸ› ï¸ Tools & Libraries Used

- **NumPy / Pandas** â€“ Data manipulation
- **Matplotlib / Seaborn** â€“ Data visualization
- **Scikit-learn** â€“ ML models and metrics
- **Plotly** â€“ Interactive visualizations (optional)

Install dependencies with:
```bash
pip install numpy pandas matplotlib seaborn scikit-learn
